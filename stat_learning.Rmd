---
title: "stat_learning"
author: "Brittany Shea"
date: "`r Sys.Date()`"
output: github_document
---

```{r, include = FALSE,message=FALSE,warning=FALSE}
library(tidyverse)
library(glmnet)

set.seed(11)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

```{r}
bwt_df = 
  read_csv("./data/birthweight.csv") %>% 
  janitor::clean_names() %>%
  mutate(
    babysex = as.factor(babysex),
    babysex = fct_recode(babysex, "male" = "1", "female" = "2"),
    frace = as.factor(frace),
    frace = fct_recode(frace, "white" = "1", "black" = "2", "asian" = "3", 
                       "puerto rican" = "4", "other" = "8"),
    malform = as.logical(malform),
    mrace = as.factor(mrace),
    mrace = fct_recode(mrace, "white" = "1", "black" = "2", "asian" = "3", 
                       "puerto rican" = "4")) %>% 
  sample_n(200)
```

`glmnet`: use for lasso

Inputs for `glmnet`: outcome vector y and design matrix (full matrix of everything that's going on). To extract matrix x, use "model.matrix" which take the design matrix out of a regression - in this instance, of birth weight on everything else.

```{r}
x = model.matrix(bwt ~ ., bwt_df)[,-1] # leave off first "intercept column"
y = bwt_df$bwt
```

### Fit lasso!

Lambda is tuning parameter that balances a residual sum of squares with a penalty on the size of the coefficients.

Degree of freedom: # of predictors that remain in model for lambda value

Big lambda, nothing in model (small df). When lambda really small, everything in model (big df).

```{r}
lambda = 10^(seq(3, -2, -0.1)) # defining grid lambda of tuning parameter values

lasso_fit =
  glmnet(x, y, lambda = lambda)

lasso_cv =
  cv.glmnet(x, y, lambda = lambda) # to choose right lambda, use cross validation; cv.glmnet does all the cross validation

lambda_opt = lasso_cv$lambda.min
```

This is the plot you see for lasso

All of the coefficients end up being shrunk exactly equal to zero. Start with coefficients and as lambda gets bigger, the penalty outweighs the residual sum of squares and shrink coefficients down toward zero.

Line shows optimal lambda from cross validation. 

```{r}
broom::tidy(lasso_fit) %>% 
  select(term, lambda, estimate) %>% 
  complete(term, lambda, fill = list(estimate = 0) ) %>% 
  filter(term != "(Intercept)") %>% 
  ggplot(aes(x = log(lambda, 10), y = estimate, group = term, color = term)) + 
  geom_path() + 
  geom_vline(xintercept = log(lambda_opt, 10), color = "blue", size = 1.2) +
  theme(legend.position = "none")
```

## Clustering

```{r}
poke_df = 
  read_csv("./data/pokemon.csv") %>% 
  janitor::clean_names() %>% 
  select(hp, speed)
```

Dont' see obvious clusters here:
```{r}
poke_df %>% 
  ggplot(aes(x = hp, y = speed)) + 
  geom_point()
```

Let's run K means - 3 means/groups to come out of this

```{r}
kmeans_fit =
  kmeans(x = poke_df, centers = 3)
```

Puts pokemons into 3 groups/clusters.

```{r}
poke_df =
  broom::augment(kmeans_fit, poke_df) # take results of k means fitting process and adds cluster assignment to data frame you started with

poke_df %>% 
  ggplot(aes(x = hp, y = speed, color = .cluster)) +
  geom_point()
```


### Longitudinal data

For longitudinal data, see how people over time (trajectory of how a person changes over time)

```{r}
traj_data = 
  read_csv("./data/trajectories.csv")
```

```{r}
traj_data %>% 
  ggplot(aes(x = week, y = value, group = subj)) + 
  geom_point() + 
  geom_path()
```
